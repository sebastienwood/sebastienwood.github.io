---
title: "The art of pruning"
categories: optimization pruning
date:   2019-05-19 11:30:19 -0400
---

## Pruning is awesome

We saw in previous (unfinished :')) posts that there are a few interesting way to optimize a Deep Neural Network resources req.

One technique has picked my interest lately thanks to the ICLR19 best paper on the Lottery Ticket Hypothesis (LTH). 

LTH is a theory on learning in NN and a technique to prune based on this theory. 

When using pruning, the usual notation is as follow :
- if we consider the parameters $\theta$ that are making the network (eg real valued tensors),
- we can define $m \in \{0,1\}^{|\theta|}$ a mask (binary tensors of the same size),
- pruning can thus be viewed as the Hadamard product (element wise product) $m \odot \theta$ that multiplies each entry of the mask $m$ with its counterpart in $\theta$.

# Reviewing LTH
## [The Lottery Ticket Hypothesis : finding sparse, trainable neural networks (Frankle and Carbin, 2019)][1]
LTH statement is straightforward : there exists a subnetwork that has a "winning ticket" that is the sole reason the whole network is performing. Other had less luck at initialization or so it seems. Which means we can remove all the useless parts of the network to boost performance (less neurons means less compute after all). To discover this subnetwork, train to maximum accuracy $\psi$ the whole network. Then prune a proportion of it, and retrain it from scratch using the exact same initialization of weights. At that point, if you reach the same accuracy $\psi$ its good job : the pruning did not hurt the subnetwork of interest. Otherwise, repeat the process. And if you feel like a superhero, you can reapply this technique till you no longer manage to find an improved pruning : you are hypothetically closer to the optimal subnetwork than you where at first.

>The Lottery Ticket Hypothesis
>A randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations. [1]

One interesting key feature is that for the pruned network it is possible to perform even better on the test set. The LTH is building upon the observations of previous years papers that it may be difficult to train a network with less weights (AKA the pruned network) with randomly initialized weights : people then agreed that keeping the final weights of the pruned network was a good practice. 
 
| **Pros** | **Cons** |
|:----:|:----:|
|The theory is appealing|Iterative pruning could take a long time|
|Address the issue of evergrowing networks (for now)|Need a retrain at each iteration (hello you GAN lover)|

Overall, it means we could save a ton of compute by *simply* having the right initialization on a less resources hungry network : there are loss regions that the optimization process can't reach through gradient descent, but you hope to be in the good starting point to actually reach it.

# [Deconstructing Lottery Tickets : Zeros, Signs and the Supermask][2]
## (Zhou et al. 2019) 



## Pytorch time
Not yet :smile:

# Pruning review
Apart from LTH which is so shiny I had to start with it, pruning is an old idea. Here I review the essential ideas/papers.

But not yet :smile:

[1]: https://arxiv.org/pdf/1803.03635.pdf
[2]: https://arxiv.org/pdf/1905.01067.pdf

*This article is WIP*