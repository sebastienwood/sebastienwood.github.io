---
title: "The art of pruning"
categories: optimization pruning
date:   2019-05-19 11:30:19 -0400
---

## Pruning is awesome

We saw in previous (unfinished :')) posts that there are a few interesting way to optimize a Deep Neural Network resources req.

One technique has picked my interest lately thanks to the ICLR19 best paper on the Lottery Ticket Hypothesis (LTH). 

LTH is a theory on learning in NN and a technique to prune based on this theory.

# Reviewing LTH
LTH statement is straightforward : there exists a subnetwork that has a "winning ticket" that is the sole reason the whole network is performing. Other had less luck at initialization or so it seems.

Which means we can remove all the useless parts of the network to boost performance (less neurons means less compute after all).

To discover this subnetwork, train to maximum accuracy psi the whole network. Then prune a proportion of it. 

At that point, if you reach the same accuracy psi its good job : the pruning did not hurt the subnetwork of interest. Otherwise, repeat the process. 

And if you feel like a superhero, you can reapply this technique till you no longer manage to find an improved pruning : you are hypothetically closer to the optimal subnetwork than you where at first.
 
| **Pros** | **Cons** |
|:----:|:----:|
|The theory is appealing|Iterative pruning could take a long time|
|Address the issue of evergrowing networks (for now)|Need a retrain at each iteration (hello you GAN lover)|

Overall, it means we could save a ton of compute by *simply* having the right initialization on a less resources hungry network : there are loss regions that the optimization process can't reach through gradient descent, but you hope to be in the good starting point to actually reach it.

## Pytorch time
Not yet :smile:

# Pruning review
Apart from LTH which is so shiny I had to start with it, pruning is an old idea. Here I review the essential ideas/papers.

But not yet :smile:

*This article is WIP*