<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-27T18:45:36-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Notes to self</title><subtitle>A collection of thoughts, concepts and tutorials oriented for my future me usage</subtitle><entry><title type="html">Advanced tricks in Deep Learnings</title><link href="http://localhost:4000/AdvancedTricks/" rel="alternate" type="text/html" title="Advanced tricks in Deep Learnings" /><published>2019-04-13T00:00:00-04:00</published><updated>2019-04-13T00:00:00-04:00</updated><id>http://localhost:4000/AdvancedTricks</id><content type="html" xml:base="http://localhost:4000/AdvancedTricks/">&lt;h1 id=&quot;what-you-wish-you-wouldve-known&quot;&gt;What you wish you would’ve known&lt;/h1&gt;

&lt;p&gt;Having taken one Deep Learning class at Polytechnique Montreal with a general approach for Deep Learning, I admit you can’t expect to review every trick to overpower your model in such a limited amount of time.&lt;/p&gt;

&lt;p&gt;However, after a torough year on Twitter (where the Deep Learning community is quite lively) and on other venues like Medium, conferences streams, and such; I’ve come to realise there are quite a lot of tricks one can think of when building a Neural Network. Those cover a wide range of concepts, from purely mathematical to hardware based tweaks.&lt;/p&gt;

&lt;p&gt;In this post, I’ll give some general ideas and classify them in Table 1 so one could have a quick answer for a given need.&lt;/p&gt;

&lt;h1 id=&quot;the-mighty-table-1&quot;&gt;The (Mighty) Table 1&lt;/h1&gt;

&lt;p&gt;¯_(ツ)_/¯&lt;/p&gt;

&lt;h1 id=&quot;the-compendium-of-network-tricks&quot;&gt;The Compendium of Network Tricks&lt;/h1&gt;

&lt;h2 id=&quot;separable-convolutions&quot;&gt;Separable Convolutions&lt;/h2&gt;

&lt;p&gt;Simple and wide spread idea of using two 1d filters to represent one 2d filter. Eg. for a 3x3 filter, you could use a 3x1 filter followed by a 1x3 filter to have the same result. Computerphile made a great video about this : https://www.youtube.com/watch?v=SiJpkucGa1o.&lt;/p&gt;

&lt;p&gt;In the usual 3x3 filter example, this leads to 6 multiplications per convolution op instead of 9. As the filter grows in size, it gets really interesting in terms of computational savings. If you’re interested in reducing the nb of parameters, it also means you only have to store 6 weights instead of 9.&lt;/p&gt;

&lt;p&gt;Unfortunately, not all filters can be &lt;em&gt;separated&lt;/em&gt;. The famous Sobel filter for edge detection is an example of a separable filter.&lt;/p&gt;

&lt;h3 id=&quot;depthwise-separable-convolutions-to-the-rescue&quot;&gt;Depthwise Separable Convolutions to the rescue&lt;/h3&gt;

&lt;p&gt;If we want to abstract the idea of separability, Mobile Nets introduced the idea of using&lt;/p&gt;

&lt;h1 id=&quot;a-wrap-up&quot;&gt;A wrap up&lt;/h1&gt;

&lt;p&gt;The community of Deep Learning offer an infinite source of tricks to enhance your network. You can find an hopefully interesting sample of those in this post.&lt;/p&gt;

&lt;p&gt;Feel free to give me a feedback, like a tricks that is missing from this list and you would like to see up there ! :)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is WIP&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">What you wish you would’ve known</summary></entry><entry><title type="html">The challenge for fewer bits in Deep Learning</title><link href="http://localhost:4000/BinaryNetworks/" rel="alternate" type="text/html" title="The challenge for fewer bits in Deep Learning" /><published>2019-03-31T00:00:00-04:00</published><updated>2019-03-31T00:00:00-04:00</updated><id>http://localhost:4000/BinaryNetworks</id><content type="html" xml:base="http://localhost:4000/BinaryNetworks/">&lt;p&gt;Usual deep learning parameters are stored on 32 bits FP hardware (though 16 bits has became supported by Pytorch, hopefully things are on a good way !).&lt;/p&gt;

&lt;p&gt;Fewer bits will reduce by a huge margin the total resources requirements : memory, compute, communication between hardware parts. In the extreme case of binarization, demanding multiplication and/or division may become bitwise shifts (moving the binary representation to the left or the right).&lt;/p&gt;

&lt;p&gt;The main use case is for edge computing or low resources hardware : with those computational savings, running the network is less demanding. The battery, often an issue, has less strain.&lt;/p&gt;

&lt;p&gt;Another use case would be to scale properly the needs for datacenters.&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Quantization&lt;/em&gt; : putting things in bins (AKA discretization). Useful to switch from a mulitple bits/bytes representation to fewer bits/bytes. Binarization is a quantization on 1 bit (2 possible states).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Normalized energy&lt;/em&gt; : to better understand the gains of a quantization tweak regarding energy, we need to compare what’s comparable. The network should be considered as a whole and thus the given performance (eg. accuracy) should be interpreted under the light of the global energy consumption.&lt;/p&gt;

&lt;p&gt;Consider for example a 1 bit quantization scheme that would degrade the performance by a margin. The same performance could be achieved by reducing the network depth, hence reducing the memory/energy/throughput by a proportional amount.&lt;/p&gt;

&lt;p&gt;Considering normalized energy as a metric is appropriate for a rigorous scientific approach.&lt;/p&gt;

&lt;h1 id=&quot;architecture-review&quot;&gt;Architecture review&lt;/h1&gt;

&lt;h2 id=&quot;binary-connect-courbariaux--al-in-2015&quot;&gt;Binary Connect (Courbariaux &amp;amp; al. in 2015)&lt;/h2&gt;

&lt;p&gt;From 32 FP weights to 1 bit weights. Using 32x less memory at run time. At training, makes use of FP representation to update the weights. Then reduce the weights to the set {-1, 1} with the sign(weight) function for all forward passes.&lt;/p&gt;

&lt;p&gt;The set with negative and positive values will allow the network to use ReLU activations functions. “Naïve” binary (0, 1) would otherwise cause all the values to be positive eventually (rendering ReLU effectless). It would require tricks like batch normalization to keep negative values.&lt;/p&gt;

&lt;p&gt;It will reduce the diversity of filters for a convolution to a combination of 2 possibles states on the size of the filter. A 3x3 filter will then have 2^(3x3)=512 possible states.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What if it used separable convolutions ? 2^(3+3)=64 possibles states. Would the result keep up ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Results are mostly good if the network is “big” enough. Savings are hard to see with Pytorch since a weight bitwidth cannot be defined out the scope 16/32bits supported natively. Some memory saving may be achieveable would the binarized weights be saved, hence discarding the real valued weights. It’s unclear if doing so would harm potential post-treatments.&lt;/p&gt;

&lt;p&gt;Computationnal savings would be scarce since the convolutions are still operating on FP inputs.&lt;/p&gt;

&lt;h2 id=&quot;dorefa-net-zhou--al-in-2016&quot;&gt;DoReFa-Net (Zhou &amp;amp; al. in 2016)&lt;/h2&gt;

&lt;p&gt;Has a more holistic approach for low bitwidth networks : quantization is applied to activations and gradients as well. DoReFa stands for the 1-bit weight, 2-bits activation and 6-bits gradients that the authors used on an Alexnet experiment with the ImageNet dataset (an octave double the frequency, whereas they double the bitwidth. &lt;strong&gt;GOOD PUN&lt;/strong&gt;). Authors have shown that a performance comparable to the 32-bits network was possible on the SVHN and ImageNet dataset. A comparison is made for a few handpicked combinations and it seems it was the most interesting.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Combinatorial choices had to be made, but the approach seems rather heuristically-guided. No normalized energy-like measure is proposed, which tends to blur the conclusions we could make.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Weights and activations make use of a deterministic quantization scheme; while the gradients “need to be stochastically quantized”. First and last layers are kept out of the optimization they propose since people tend to agree doing so would tends to degrade “too much” (without a measure of “how much”) the network performance.&lt;/p&gt;

&lt;p&gt;Authors propose a quantization scheme as follow :&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Binary-Net&lt;/em&gt; :&lt;/p&gt;

&lt;p&gt;*XNOR-Net :&lt;/p&gt;

&lt;h1 id=&quot;pytorch-time&quot;&gt;PyTorch time&lt;/h1&gt;

&lt;p&gt;3 paths I know of : external weight manager (an object called at will to quantize), customs layers which autoregulate themselves, or forward/backwards hooks.&lt;/p&gt;

&lt;h2 id=&quot;external-weight-manager&quot;&gt;External Weight Manager&lt;/h2&gt;
&lt;p&gt;Can be applied to any nn.Module on the fly. Severe limitations if we want to modify in-depth the behavior of e.g. grads : would require far more code to do the same results as path 2.&lt;/p&gt;

&lt;p&gt;’'’some_tensor.data.copy_’’’ allow to modify in place a Pytorch tensor value.
Logic may be included in the quantization function, e.g. applying to Conv2d layers only. Would need to test additionnal attributes at model initialization to diversify the logics used. E.g. for ResNet, differentiating blocks numbers.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EWM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      	  &lt;span class=&quot;c&quot;&gt;# grab the pointer to the real weights&lt;/span&gt;
	  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
	  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
	  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# iterate over the layers&lt;/span&gt;
	      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real_weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored_weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;quantize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      	  &lt;span class=&quot;c&quot;&gt;# store the real weight and modify in place the actual weights&lt;/span&gt;
	  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	      &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;custom-layers&quot;&gt;Custom layers&lt;/h2&gt;
&lt;p&gt;Require to redefine each architecture using the custom layers. Can do anything.&lt;/p&gt;

&lt;h2 id=&quot;forwardbackward-hooks&quot;&gt;Forward/backward hooks&lt;/h2&gt;
&lt;p&gt;Pytorch offers to register forward and backwards hooks on a model. Pytorch documentation for this method is parse/non-existent.&lt;/p&gt;

&lt;p&gt;It appears that Pytorch forward hook doesn’t allow on the fly modification to the output tensor (e.g. applying our  quantization function would fail). However the backward hooks allow to modify the grad tensor (developer logic ?).&lt;/p&gt;

&lt;p&gt;Given https://github.com/pytorch/pytorch/issues/262 it seems that it’s just awaiting a courageous developer to implement a functional forward hook. L-&lt;em&gt;hook forward&lt;/em&gt; to it !&lt;/p&gt;

&lt;h2 id=&quot;diving-deeper&quot;&gt;Diving deeper&lt;/h2&gt;
&lt;p&gt;If you want to optimize the actual hardware ops (CUDA 1 bit, etc) while it’s not supported officially by either Pytorch or NVIDIA, you’re bound to write custom kernels (a kernel is just a piece of software for CUDA) and custom Pytorch OPS. Cross your fingers for backward compatibility in updates of those frameworks.&lt;/p&gt;

&lt;p&gt;Otherwise, FPGA (a customizable piece of hardware) can be used too, mostly if you’re planning to use it for inference only (may be overly complicated either way).&lt;/p&gt;

&lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;You can effectively quantize the building blocs of a neural networks and still manage to reach a good accuracy.&lt;/p&gt;

&lt;p&gt;There are diverse way to quantize the blocs, with the challenging objective to reduce as much as possible the bits used while keeping a high accuracy. The inherent quantization noise can act as a regulariser.&lt;/p&gt;

&lt;p&gt;It all depends on the context for the results, and the field is still too novel to ensure one basic quantization scheme will work flawlessly across all models. I personnally would tend to think overly large networks (i.e. many parameters)  would manage to work in a quantized context. The real challenge lies in quantizing small enough networks.&lt;/p&gt;

&lt;p&gt;Small enough networks echoes the fact that the same energy savings could be achieved by only reducing the network size. It thus matter to compare the energy at a constant accuracy (AKA normalized energy) so as to have more meaningful hindsights toward the &lt;em&gt;real&lt;/em&gt; value of the quantization scheme proposed.&lt;/p&gt;

&lt;p&gt;In the end, you just reduce the amount of information (or reduce the entropy) of the network.&lt;/p&gt;

&lt;h1 id=&quot;open-questions&quot;&gt;Open questions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;what are the necessary conditions for a quantization scheme to work ?&lt;/li&gt;
  &lt;li&gt;how would it cope in contexts of smalls tweaks like separable convolutions ?&lt;/li&gt;
  &lt;li&gt;given the finite combination of filters for a binarized CNN, what can we learn about the dynamics of a neural networks ? (for that last sentence, works like Capsule Network (Hinton) and the general criticism of current SotA Deep Learning seems pretty relevant)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;This article is a WIP&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feel free to contact me on Twitter if you have any question/remark/suggestion&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Usual deep learning parameters are stored on 32 bits FP hardware (though 16 bits has became supported by Pytorch, hopefully things are on a good way !).</summary></entry></feed>